{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FFNNEq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPplDSIBaQ/9mEXQXQmFFy0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcash8/DVBS2X-DNN-Implementation/blob/main/FFNNEq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "d4hfED18HHhd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7bdfbec-e352-4b8b-ab82-9455b5e91d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data_BO1.mat\n",
            "(101500, 18) train samples\n",
            "(1015, 18) valid samples\n",
            "(101500, 2) train labels\n",
            "(1015, 2) valid labels\n",
            "Label Examples:\n",
            " [[ 0.22448 -0.22448]\n",
            " [-0.22448  0.22448]\n",
            " [-0.70711  0.70711]\n",
            " [-0.22448  0.22448]\n",
            " [-0.25882  0.96593]\n",
            " [ 0.70711 -0.70711]\n",
            " [ 0.96593  0.25882]\n",
            " [ 0.25882  0.96593]\n",
            " [-0.22448 -0.22448]]\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_24 (Dense)            (None, 50)                950       \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 50)                2550      \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 2)                 102       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,602\n",
            "Trainable params: 3,602\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "1586/1586 - 5s - loss: 0.3845 - rmse: 0.6201 - val_loss: 0.3661 - val_rmse: 0.6051 - 5s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1586/1586 - 4s - loss: 0.3548 - rmse: 0.5957 - val_loss: 0.3437 - val_rmse: 0.5863 - 4s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1586/1586 - 4s - loss: 0.3328 - rmse: 0.5769 - val_loss: 0.3212 - val_rmse: 0.5667 - 4s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1586/1586 - 4s - loss: 0.3102 - rmse: 0.5569 - val_loss: 0.2984 - val_rmse: 0.5462 - 4s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1586/1586 - 4s - loss: 0.2873 - rmse: 0.5360 - val_loss: 0.2753 - val_rmse: 0.5247 - 4s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1586/1586 - 4s - loss: 0.2648 - rmse: 0.5146 - val_loss: 0.2533 - val_rmse: 0.5033 - 4s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1586/1586 - 4s - loss: 0.2425 - rmse: 0.4924 - val_loss: 0.2307 - val_rmse: 0.4803 - 4s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1586/1586 - 4s - loss: 0.2208 - rmse: 0.4699 - val_loss: 0.2097 - val_rmse: 0.4579 - 4s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1586/1586 - 4s - loss: 0.2005 - rmse: 0.4478 - val_loss: 0.1903 - val_rmse: 0.4362 - 4s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1586/1586 - 4s - loss: 0.1820 - rmse: 0.4266 - val_loss: 0.1727 - val_rmse: 0.4156 - 4s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1586/1586 - 4s - loss: 0.1655 - rmse: 0.4068 - val_loss: 0.1578 - val_rmse: 0.3973 - 4s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1586/1586 - 4s - loss: 0.1507 - rmse: 0.3881 - val_loss: 0.1445 - val_rmse: 0.3801 - 4s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1586/1586 - 4s - loss: 0.1373 - rmse: 0.3705 - val_loss: 0.1309 - val_rmse: 0.3617 - 4s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1586/1586 - 4s - loss: 0.1252 - rmse: 0.3538 - val_loss: 0.1194 - val_rmse: 0.3456 - 4s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1586/1586 - 4s - loss: 0.1143 - rmse: 0.3381 - val_loss: 0.1087 - val_rmse: 0.3297 - 4s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1586/1586 - 4s - loss: 0.1044 - rmse: 0.3231 - val_loss: 0.1000 - val_rmse: 0.3163 - 4s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1586/1586 - 4s - loss: 0.0954 - rmse: 0.3089 - val_loss: 0.0908 - val_rmse: 0.3014 - 4s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1586/1586 - 4s - loss: 0.0873 - rmse: 0.2955 - val_loss: 0.0832 - val_rmse: 0.2884 - 4s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1586/1586 - 4s - loss: 0.0801 - rmse: 0.2831 - val_loss: 0.0766 - val_rmse: 0.2767 - 4s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1586/1586 - 4s - loss: 0.0736 - rmse: 0.2713 - val_loss: 0.0701 - val_rmse: 0.2648 - 4s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1586/1586 - 4s - loss: 0.0676 - rmse: 0.2601 - val_loss: 0.0647 - val_rmse: 0.2543 - 4s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1586/1586 - 4s - loss: 0.0622 - rmse: 0.2494 - val_loss: 0.0593 - val_rmse: 0.2435 - 4s/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "1586/1586 - 4s - loss: 0.0572 - rmse: 0.2392 - val_loss: 0.0547 - val_rmse: 0.2340 - 4s/epoch - 3ms/step\n",
            "Epoch 24/100\n",
            "1586/1586 - 4s - loss: 0.0527 - rmse: 0.2295 - val_loss: 0.0507 - val_rmse: 0.2251 - 4s/epoch - 3ms/step\n",
            "Epoch 25/100\n",
            "1586/1586 - 4s - loss: 0.0485 - rmse: 0.2202 - val_loss: 0.0462 - val_rmse: 0.2149 - 4s/epoch - 3ms/step\n",
            "Epoch 26/100\n",
            "1586/1586 - 4s - loss: 0.0447 - rmse: 0.2114 - val_loss: 0.0430 - val_rmse: 0.2073 - 4s/epoch - 3ms/step\n",
            "Epoch 27/100\n",
            "1586/1586 - 4s - loss: 0.0413 - rmse: 0.2032 - val_loss: 0.0394 - val_rmse: 0.1986 - 4s/epoch - 3ms/step\n",
            "Epoch 28/100\n",
            "1586/1586 - 4s - loss: 0.0381 - rmse: 0.1953 - val_loss: 0.0365 - val_rmse: 0.1910 - 4s/epoch - 3ms/step\n",
            "Epoch 29/100\n",
            "1586/1586 - 4s - loss: 0.0353 - rmse: 0.1878 - val_loss: 0.0338 - val_rmse: 0.1839 - 4s/epoch - 3ms/step\n",
            "Epoch 30/100\n",
            "1586/1586 - 4s - loss: 0.0327 - rmse: 0.1807 - val_loss: 0.0310 - val_rmse: 0.1761 - 4s/epoch - 3ms/step\n",
            "Epoch 31/100\n",
            "1586/1586 - 4s - loss: 0.0302 - rmse: 0.1739 - val_loss: 0.0291 - val_rmse: 0.1705 - 4s/epoch - 3ms/step\n",
            "Epoch 32/100\n",
            "1586/1586 - 4s - loss: 0.0281 - rmse: 0.1675 - val_loss: 0.0271 - val_rmse: 0.1646 - 4s/epoch - 3ms/step\n",
            "Epoch 33/100\n",
            "1586/1586 - 4s - loss: 0.0261 - rmse: 0.1614 - val_loss: 0.0249 - val_rmse: 0.1579 - 4s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1586/1586 - 4s - loss: 0.0242 - rmse: 0.1556 - val_loss: 0.0231 - val_rmse: 0.1521 - 4s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1586/1586 - 4s - loss: 0.0225 - rmse: 0.1500 - val_loss: 0.0216 - val_rmse: 0.1470 - 4s/epoch - 3ms/step\n",
            "Epoch 36/100\n",
            "1586/1586 - 4s - loss: 0.0210 - rmse: 0.1448 - val_loss: 0.0202 - val_rmse: 0.1420 - 4s/epoch - 3ms/step\n",
            "Epoch 37/100\n",
            "1586/1586 - 4s - loss: 0.0195 - rmse: 0.1398 - val_loss: 0.0188 - val_rmse: 0.1370 - 4s/epoch - 3ms/step\n",
            "Epoch 38/100\n",
            "1586/1586 - 4s - loss: 0.0182 - rmse: 0.1350 - val_loss: 0.0175 - val_rmse: 0.1324 - 4s/epoch - 3ms/step\n",
            "Epoch 39/100\n",
            "1586/1586 - 4s - loss: 0.0170 - rmse: 0.1304 - val_loss: 0.0163 - val_rmse: 0.1275 - 4s/epoch - 3ms/step\n",
            "Epoch 40/100\n",
            "1586/1586 - 4s - loss: 0.0159 - rmse: 0.1260 - val_loss: 0.0153 - val_rmse: 0.1238 - 4s/epoch - 3ms/step\n",
            "Epoch 41/100\n",
            "1586/1586 - 4s - loss: 0.0149 - rmse: 0.1219 - val_loss: 0.0143 - val_rmse: 0.1195 - 4s/epoch - 3ms/step\n",
            "Epoch 42/100\n",
            "1586/1586 - 4s - loss: 0.0139 - rmse: 0.1179 - val_loss: 0.0133 - val_rmse: 0.1155 - 4s/epoch - 3ms/step\n",
            "Epoch 43/100\n",
            "1586/1586 - 4s - loss: 0.0130 - rmse: 0.1141 - val_loss: 0.0125 - val_rmse: 0.1119 - 4s/epoch - 3ms/step\n",
            "Epoch 44/100\n",
            "1586/1586 - 4s - loss: 0.0122 - rmse: 0.1105 - val_loss: 0.0117 - val_rmse: 0.1082 - 4s/epoch - 3ms/step\n",
            "Epoch 45/100\n",
            "1586/1586 - 4s - loss: 0.0114 - rmse: 0.1070 - val_loss: 0.0111 - val_rmse: 0.1054 - 4s/epoch - 3ms/step\n",
            "Epoch 46/100\n",
            "1586/1586 - 4s - loss: 0.0107 - rmse: 0.1037 - val_loss: 0.0105 - val_rmse: 0.1025 - 4s/epoch - 3ms/step\n",
            "Epoch 47/100\n",
            "1586/1586 - 4s - loss: 0.0101 - rmse: 0.1004 - val_loss: 0.0097 - val_rmse: 0.0983 - 4s/epoch - 3ms/step\n",
            "Epoch 48/100\n",
            "1586/1586 - 4s - loss: 0.0095 - rmse: 0.0973 - val_loss: 0.0091 - val_rmse: 0.0956 - 4s/epoch - 3ms/step\n",
            "Epoch 49/100\n",
            "1586/1586 - 4s - loss: 0.0089 - rmse: 0.0944 - val_loss: 0.0086 - val_rmse: 0.0929 - 4s/epoch - 3ms/step\n",
            "Epoch 50/100\n",
            "1586/1586 - 4s - loss: 0.0084 - rmse: 0.0915 - val_loss: 0.0081 - val_rmse: 0.0898 - 4s/epoch - 3ms/step\n",
            "Epoch 51/100\n",
            "1586/1586 - 4s - loss: 0.0079 - rmse: 0.0888 - val_loss: 0.0076 - val_rmse: 0.0873 - 4s/epoch - 3ms/step\n",
            "Epoch 52/100\n",
            "1586/1586 - 4s - loss: 0.0074 - rmse: 0.0862 - val_loss: 0.0072 - val_rmse: 0.0850 - 4s/epoch - 3ms/step\n",
            "Epoch 53/100\n",
            "1586/1586 - 4s - loss: 0.0070 - rmse: 0.0837 - val_loss: 0.0068 - val_rmse: 0.0824 - 4s/epoch - 3ms/step\n",
            "Epoch 54/100\n",
            "1586/1586 - 4s - loss: 0.0066 - rmse: 0.0813 - val_loss: 0.0064 - val_rmse: 0.0799 - 4s/epoch - 3ms/step\n",
            "Epoch 55/100\n",
            "1586/1586 - 4s - loss: 0.0062 - rmse: 0.0790 - val_loss: 0.0060 - val_rmse: 0.0775 - 4s/epoch - 3ms/step\n",
            "Epoch 56/100\n",
            "1586/1586 - 4s - loss: 0.0059 - rmse: 0.0767 - val_loss: 0.0057 - val_rmse: 0.0758 - 4s/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "1586/1586 - 4s - loss: 0.0056 - rmse: 0.0746 - val_loss: 0.0054 - val_rmse: 0.0735 - 4s/epoch - 3ms/step\n",
            "Epoch 58/100\n",
            "1586/1586 - 4s - loss: 0.0053 - rmse: 0.0725 - val_loss: 0.0051 - val_rmse: 0.0713 - 4s/epoch - 3ms/step\n",
            "Epoch 59/100\n",
            "1586/1586 - 4s - loss: 0.0050 - rmse: 0.0705 - val_loss: 0.0048 - val_rmse: 0.0694 - 4s/epoch - 3ms/step\n",
            "Epoch 60/100\n",
            "1586/1586 - 4s - loss: 0.0047 - rmse: 0.0686 - val_loss: 0.0046 - val_rmse: 0.0678 - 4s/epoch - 3ms/step\n",
            "Epoch 61/100\n",
            "1586/1586 - 4s - loss: 0.0045 - rmse: 0.0668 - val_loss: 0.0043 - val_rmse: 0.0659 - 4s/epoch - 3ms/step\n",
            "Epoch 62/100\n",
            "1586/1586 - 4s - loss: 0.0042 - rmse: 0.0650 - val_loss: 0.0041 - val_rmse: 0.0640 - 4s/epoch - 3ms/step\n",
            "Epoch 63/100\n",
            "1586/1586 - 4s - loss: 0.0040 - rmse: 0.0633 - val_loss: 0.0039 - val_rmse: 0.0622 - 4s/epoch - 3ms/step\n",
            "Epoch 64/100\n",
            "1586/1586 - 4s - loss: 0.0038 - rmse: 0.0617 - val_loss: 0.0037 - val_rmse: 0.0606 - 4s/epoch - 3ms/step\n",
            "Epoch 65/100\n",
            "1586/1586 - 4s - loss: 0.0036 - rmse: 0.0601 - val_loss: 0.0035 - val_rmse: 0.0590 - 4s/epoch - 3ms/step\n",
            "Epoch 66/100\n",
            "1586/1586 - 4s - loss: 0.0034 - rmse: 0.0585 - val_loss: 0.0033 - val_rmse: 0.0577 - 4s/epoch - 3ms/step\n",
            "Epoch 67/100\n",
            "1586/1586 - 4s - loss: 0.0033 - rmse: 0.0570 - val_loss: 0.0032 - val_rmse: 0.0562 - 4s/epoch - 3ms/step\n",
            "Epoch 68/100\n",
            "1586/1586 - 4s - loss: 0.0031 - rmse: 0.0556 - val_loss: 0.0030 - val_rmse: 0.0549 - 4s/epoch - 3ms/step\n",
            "Epoch 69/100\n",
            "1586/1586 - 4s - loss: 0.0029 - rmse: 0.0542 - val_loss: 0.0028 - val_rmse: 0.0534 - 4s/epoch - 3ms/step\n",
            "Epoch 70/100\n",
            "1586/1586 - 4s - loss: 0.0028 - rmse: 0.0529 - val_loss: 0.0027 - val_rmse: 0.0520 - 4s/epoch - 3ms/step\n",
            "Epoch 71/100\n",
            "1586/1586 - 4s - loss: 0.0027 - rmse: 0.0516 - val_loss: 0.0026 - val_rmse: 0.0507 - 4s/epoch - 3ms/step\n",
            "Epoch 72/100\n",
            "1586/1586 - 4s - loss: 0.0025 - rmse: 0.0503 - val_loss: 0.0025 - val_rmse: 0.0497 - 4s/epoch - 3ms/step\n",
            "Epoch 73/100\n",
            "1586/1586 - 4s - loss: 0.0024 - rmse: 0.0491 - val_loss: 0.0023 - val_rmse: 0.0484 - 4s/epoch - 3ms/step\n",
            "Epoch 74/100\n",
            "1586/1586 - 4s - loss: 0.0023 - rmse: 0.0479 - val_loss: 0.0022 - val_rmse: 0.0473 - 4s/epoch - 3ms/step\n",
            "Epoch 75/100\n",
            "1586/1586 - 4s - loss: 0.0022 - rmse: 0.0468 - val_loss: 0.0021 - val_rmse: 0.0460 - 4s/epoch - 3ms/step\n",
            "Epoch 76/100\n",
            "1586/1586 - 4s - loss: 0.0021 - rmse: 0.0457 - val_loss: 0.0020 - val_rmse: 0.0451 - 4s/epoch - 3ms/step\n",
            "Epoch 77/100\n",
            "1586/1586 - 4s - loss: 0.0020 - rmse: 0.0446 - val_loss: 0.0019 - val_rmse: 0.0438 - 4s/epoch - 3ms/step\n",
            "Epoch 78/100\n",
            "1586/1586 - 4s - loss: 0.0019 - rmse: 0.0436 - val_loss: 0.0018 - val_rmse: 0.0429 - 4s/epoch - 3ms/step\n",
            "Epoch 79/100\n",
            "1586/1586 - 4s - loss: 0.0018 - rmse: 0.0426 - val_loss: 0.0018 - val_rmse: 0.0420 - 4s/epoch - 3ms/step\n",
            "Epoch 80/100\n",
            "1586/1586 - 4s - loss: 0.0017 - rmse: 0.0416 - val_loss: 0.0017 - val_rmse: 0.0409 - 4s/epoch - 3ms/step\n",
            "Epoch 81/100\n",
            "1586/1586 - 4s - loss: 0.0017 - rmse: 0.0406 - val_loss: 0.0016 - val_rmse: 0.0399 - 4s/epoch - 3ms/step\n",
            "Epoch 82/100\n",
            "1586/1586 - 4s - loss: 0.0016 - rmse: 0.0397 - val_loss: 0.0015 - val_rmse: 0.0393 - 4s/epoch - 3ms/step\n",
            "Epoch 83/100\n",
            "1586/1586 - 4s - loss: 0.0015 - rmse: 0.0388 - val_loss: 0.0015 - val_rmse: 0.0382 - 4s/epoch - 3ms/step\n",
            "Epoch 84/100\n",
            "1586/1586 - 4s - loss: 0.0014 - rmse: 0.0379 - val_loss: 0.0014 - val_rmse: 0.0376 - 4s/epoch - 3ms/step\n",
            "Epoch 85/100\n",
            "1586/1586 - 4s - loss: 0.0014 - rmse: 0.0371 - val_loss: 0.0013 - val_rmse: 0.0365 - 4s/epoch - 3ms/step\n",
            "Epoch 86/100\n",
            "1586/1586 - 4s - loss: 0.0013 - rmse: 0.0362 - val_loss: 0.0013 - val_rmse: 0.0358 - 4s/epoch - 3ms/step\n",
            "Epoch 87/100\n",
            "1586/1586 - 4s - loss: 0.0013 - rmse: 0.0355 - val_loss: 0.0012 - val_rmse: 0.0351 - 4s/epoch - 3ms/step\n",
            "Epoch 88/100\n",
            "1586/1586 - 4s - loss: 0.0012 - rmse: 0.0347 - val_loss: 0.0012 - val_rmse: 0.0342 - 4s/epoch - 3ms/step\n",
            "Epoch 89/100\n",
            "1586/1586 - 4s - loss: 0.0011 - rmse: 0.0339 - val_loss: 0.0011 - val_rmse: 0.0333 - 4s/epoch - 3ms/step\n",
            "Epoch 90/100\n",
            "1586/1586 - 4s - loss: 0.0011 - rmse: 0.0332 - val_loss: 0.0011 - val_rmse: 0.0327 - 4s/epoch - 3ms/step\n",
            "Epoch 91/100\n",
            "1586/1586 - 4s - loss: 0.0011 - rmse: 0.0325 - val_loss: 0.0010 - val_rmse: 0.0320 - 4s/epoch - 3ms/step\n",
            "Epoch 92/100\n",
            "1586/1586 - 4s - loss: 0.0010 - rmse: 0.0318 - val_loss: 9.7469e-04 - val_rmse: 0.0312 - 4s/epoch - 3ms/step\n",
            "Epoch 93/100\n",
            "1586/1586 - 4s - loss: 9.6627e-04 - rmse: 0.0311 - val_loss: 9.4005e-04 - val_rmse: 0.0307 - 4s/epoch - 3ms/step\n",
            "Epoch 94/100\n",
            "1586/1586 - 4s - loss: 9.2563e-04 - rmse: 0.0304 - val_loss: 8.9401e-04 - val_rmse: 0.0299 - 4s/epoch - 3ms/step\n",
            "Epoch 95/100\n",
            "1586/1586 - 4s - loss: 8.8643e-04 - rmse: 0.0298 - val_loss: 8.5908e-04 - val_rmse: 0.0293 - 4s/epoch - 3ms/step\n",
            "Epoch 96/100\n",
            "1586/1586 - 4s - loss: 8.4919e-04 - rmse: 0.0291 - val_loss: 8.2442e-04 - val_rmse: 0.0287 - 4s/epoch - 3ms/step\n",
            "Epoch 97/100\n",
            "1586/1586 - 4s - loss: 8.1408e-04 - rmse: 0.0285 - val_loss: 7.9995e-04 - val_rmse: 0.0283 - 4s/epoch - 3ms/step\n",
            "Epoch 98/100\n",
            "1586/1586 - 4s - loss: 7.8043e-04 - rmse: 0.0279 - val_loss: 7.6284e-04 - val_rmse: 0.0276 - 4s/epoch - 3ms/step\n",
            "Epoch 99/100\n",
            "1586/1586 - 4s - loss: 7.4826e-04 - rmse: 0.0274 - val_loss: 7.2850e-04 - val_rmse: 0.0270 - 4s/epoch - 3ms/step\n",
            "Epoch 100/100\n",
            "1586/1586 - 4s - loss: 7.1769e-04 - rmse: 0.0268 - val_loss: 6.9560e-04 - val_rmse: 0.0264 - 4s/epoch - 3ms/step\n",
            "3172/3172 - 6s - loss: 6.9560e-04 - rmse: 0.0264 - 6s/epoch - 2ms/step\n",
            "Final Training MSE: 0.0006955984863452613\n",
            "Final Training RMSE: 0.026374200358986855\n",
            "32/32 - 0s - loss: 6.9560e-04 - rmse: 0.0264 - 81ms/epoch - 3ms/step\n",
            "Final Validation MSE: 0.0006955978460609913\n",
            "Final Validation RMSE: 0.02637418732047081\n",
            "--- 7.03 minutes ---\n",
            "Data_BO7.mat\n",
            "(101500, 18) train samples\n",
            "(1015, 18) valid samples\n",
            "(101500, 2) train labels\n",
            "(1015, 2) valid labels\n",
            "Label Examples:\n",
            " [[ 0.22448 -0.22448]\n",
            " [-0.22448  0.22448]\n",
            " [-0.70711  0.70711]\n",
            " [-0.22448  0.22448]\n",
            " [-0.25882  0.96593]\n",
            " [ 0.70711 -0.70711]\n",
            " [ 0.96593  0.25882]\n",
            " [ 0.25882  0.96593]\n",
            " [-0.22448 -0.22448]]\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_27 (Dense)            (None, 50)                950       \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 50)                2550      \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 2)                 102       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,602\n",
            "Trainable params: 3,602\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "1586/1586 - 5s - loss: 0.3912 - rmse: 0.6255 - val_loss: 0.3674 - val_rmse: 0.6061 - 5s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1586/1586 - 4s - loss: 0.3563 - rmse: 0.5969 - val_loss: 0.3448 - val_rmse: 0.5872 - 4s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1586/1586 - 4s - loss: 0.3344 - rmse: 0.5783 - val_loss: 0.3232 - val_rmse: 0.5685 - 4s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1586/1586 - 4s - loss: 0.3122 - rmse: 0.5588 - val_loss: 0.3001 - val_rmse: 0.5478 - 4s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1586/1586 - 4s - loss: 0.2890 - rmse: 0.5376 - val_loss: 0.2769 - val_rmse: 0.5262 - 4s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1586/1586 - 4s - loss: 0.2659 - rmse: 0.5157 - val_loss: 0.2547 - val_rmse: 0.5047 - 4s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1586/1586 - 4s - loss: 0.2438 - rmse: 0.4938 - val_loss: 0.2327 - val_rmse: 0.4824 - 4s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1586/1586 - 4s - loss: 0.2233 - rmse: 0.4725 - val_loss: 0.2135 - val_rmse: 0.4620 - 4s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1586/1586 - 4s - loss: 0.2043 - rmse: 0.4520 - val_loss: 0.1949 - val_rmse: 0.4415 - 4s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1586/1586 - 4s - loss: 0.1867 - rmse: 0.4321 - val_loss: 0.1776 - val_rmse: 0.4214 - 4s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1586/1586 - 4s - loss: 0.1700 - rmse: 0.4124 - val_loss: 0.1617 - val_rmse: 0.4021 - 4s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1586/1586 - 4s - loss: 0.1543 - rmse: 0.3928 - val_loss: 0.1463 - val_rmse: 0.3825 - 4s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1586/1586 - 4s - loss: 0.1397 - rmse: 0.3738 - val_loss: 0.1328 - val_rmse: 0.3644 - 4s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1586/1586 - 4s - loss: 0.1265 - rmse: 0.3557 - val_loss: 0.1206 - val_rmse: 0.3472 - 4s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1586/1586 - 4s - loss: 0.1145 - rmse: 0.3384 - val_loss: 0.1090 - val_rmse: 0.3302 - 4s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1586/1586 - 4s - loss: 0.1034 - rmse: 0.3216 - val_loss: 0.0976 - val_rmse: 0.3124 - 4s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1586/1586 - 4s - loss: 0.0935 - rmse: 0.3057 - val_loss: 0.0881 - val_rmse: 0.2968 - 4s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1586/1586 - 4s - loss: 0.0842 - rmse: 0.2901 - val_loss: 0.0796 - val_rmse: 0.2821 - 4s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1586/1586 - 4s - loss: 0.0759 - rmse: 0.2754 - val_loss: 0.0720 - val_rmse: 0.2683 - 4s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1586/1586 - 4s - loss: 0.0684 - rmse: 0.2616 - val_loss: 0.0648 - val_rmse: 0.2545 - 4s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1586/1586 - 4s - loss: 0.0618 - rmse: 0.2487 - val_loss: 0.0585 - val_rmse: 0.2419 - 4s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1586/1586 - 4s - loss: 0.0560 - rmse: 0.2367 - val_loss: 0.0529 - val_rmse: 0.2301 - 4s/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "1586/1586 - 4s - loss: 0.0510 - rmse: 0.2257 - val_loss: 0.0482 - val_rmse: 0.2195 - 4s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1586/1586 - 4s - loss: 0.0465 - rmse: 0.2156 - val_loss: 0.0443 - val_rmse: 0.2104 - 4s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1586/1586 - 4s - loss: 0.0425 - rmse: 0.2062 - val_loss: 0.0406 - val_rmse: 0.2015 - 4s/epoch - 3ms/step\n",
            "Epoch 26/100\n",
            "1586/1586 - 4s - loss: 0.0390 - rmse: 0.1976 - val_loss: 0.0372 - val_rmse: 0.1928 - 4s/epoch - 3ms/step\n",
            "Epoch 27/100\n",
            "1586/1586 - 4s - loss: 0.0359 - rmse: 0.1895 - val_loss: 0.0344 - val_rmse: 0.1854 - 4s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1586/1586 - 4s - loss: 0.0331 - rmse: 0.1819 - val_loss: 0.0317 - val_rmse: 0.1780 - 4s/epoch - 3ms/step\n",
            "Epoch 29/100\n",
            "1586/1586 - 4s - loss: 0.0305 - rmse: 0.1747 - val_loss: 0.0291 - val_rmse: 0.1705 - 4s/epoch - 3ms/step\n",
            "Epoch 30/100\n",
            "1586/1586 - 4s - loss: 0.0282 - rmse: 0.1680 - val_loss: 0.0271 - val_rmse: 0.1645 - 4s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1586/1586 - 4s - loss: 0.0261 - rmse: 0.1617 - val_loss: 0.0250 - val_rmse: 0.1581 - 4s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1586/1586 - 4s - loss: 0.0242 - rmse: 0.1556 - val_loss: 0.0231 - val_rmse: 0.1520 - 4s/epoch - 3ms/step\n",
            "Epoch 33/100\n",
            "1586/1586 - 4s - loss: 0.0224 - rmse: 0.1496 - val_loss: 0.0214 - val_rmse: 0.1463 - 4s/epoch - 3ms/step\n",
            "Epoch 34/100\n",
            "1586/1586 - 4s - loss: 0.0208 - rmse: 0.1440 - val_loss: 0.0198 - val_rmse: 0.1406 - 4s/epoch - 3ms/step\n",
            "Epoch 35/100\n",
            "1586/1586 - 4s - loss: 0.0192 - rmse: 0.1387 - val_loss: 0.0183 - val_rmse: 0.1354 - 4s/epoch - 3ms/step\n",
            "Epoch 36/100\n",
            "1586/1586 - 4s - loss: 0.0178 - rmse: 0.1335 - val_loss: 0.0171 - val_rmse: 0.1308 - 4s/epoch - 3ms/step\n",
            "Epoch 37/100\n",
            "1586/1586 - 4s - loss: 0.0165 - rmse: 0.1286 - val_loss: 0.0158 - val_rmse: 0.1258 - 4s/epoch - 3ms/step\n",
            "Epoch 38/100\n",
            "1586/1586 - 4s - loss: 0.0154 - rmse: 0.1239 - val_loss: 0.0148 - val_rmse: 0.1215 - 4s/epoch - 3ms/step\n",
            "Epoch 39/100\n",
            "1586/1586 - 4s - loss: 0.0143 - rmse: 0.1195 - val_loss: 0.0136 - val_rmse: 0.1166 - 4s/epoch - 3ms/step\n",
            "Epoch 40/100\n",
            "1586/1586 - 4s - loss: 0.0133 - rmse: 0.1152 - val_loss: 0.0128 - val_rmse: 0.1132 - 4s/epoch - 3ms/step\n",
            "Epoch 41/100\n",
            "1586/1586 - 4s - loss: 0.0124 - rmse: 0.1111 - val_loss: 0.0119 - val_rmse: 0.1091 - 4s/epoch - 3ms/step\n",
            "Epoch 42/100\n",
            "1586/1586 - 4s - loss: 0.0115 - rmse: 0.1073 - val_loss: 0.0110 - val_rmse: 0.1051 - 4s/epoch - 3ms/step\n",
            "Epoch 43/100\n",
            "1586/1586 - 4s - loss: 0.0107 - rmse: 0.1036 - val_loss: 0.0103 - val_rmse: 0.1015 - 4s/epoch - 3ms/step\n",
            "Epoch 44/100\n",
            "1586/1586 - 4s - loss: 0.0100 - rmse: 0.1001 - val_loss: 0.0096 - val_rmse: 0.0978 - 4s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "1586/1586 - 4s - loss: 0.0093 - rmse: 0.0967 - val_loss: 0.0090 - val_rmse: 0.0951 - 4s/epoch - 3ms/step\n",
            "Epoch 46/100\n",
            "1586/1586 - 4s - loss: 0.0087 - rmse: 0.0935 - val_loss: 0.0084 - val_rmse: 0.0916 - 4s/epoch - 3ms/step\n",
            "Epoch 47/100\n",
            "1586/1586 - 4s - loss: 0.0082 - rmse: 0.0904 - val_loss: 0.0078 - val_rmse: 0.0886 - 4s/epoch - 3ms/step\n",
            "Epoch 48/100\n",
            "1586/1586 - 4s - loss: 0.0076 - rmse: 0.0874 - val_loss: 0.0074 - val_rmse: 0.0858 - 4s/epoch - 3ms/step\n",
            "Epoch 49/100\n",
            "1586/1586 - 4s - loss: 0.0072 - rmse: 0.0846 - val_loss: 0.0069 - val_rmse: 0.0830 - 4s/epoch - 3ms/step\n",
            "Epoch 50/100\n",
            "1586/1586 - 4s - loss: 0.0067 - rmse: 0.0819 - val_loss: 0.0065 - val_rmse: 0.0805 - 4s/epoch - 3ms/step\n",
            "Epoch 51/100\n",
            "1586/1586 - 4s - loss: 0.0063 - rmse: 0.0793 - val_loss: 0.0061 - val_rmse: 0.0780 - 4s/epoch - 3ms/step\n",
            "Epoch 52/100\n",
            "1586/1586 - 4s - loss: 0.0059 - rmse: 0.0768 - val_loss: 0.0057 - val_rmse: 0.0753 - 4s/epoch - 3ms/step\n",
            "Epoch 53/100\n",
            "1586/1586 - 4s - loss: 0.0055 - rmse: 0.0744 - val_loss: 0.0053 - val_rmse: 0.0731 - 4s/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "1586/1586 - 4s - loss: 0.0052 - rmse: 0.0721 - val_loss: 0.0051 - val_rmse: 0.0711 - 4s/epoch - 3ms/step\n",
            "Epoch 55/100\n",
            "1586/1586 - 4s - loss: 0.0049 - rmse: 0.0699 - val_loss: 0.0047 - val_rmse: 0.0687 - 4s/epoch - 3ms/step\n",
            "Epoch 56/100\n",
            "1586/1586 - 4s - loss: 0.0046 - rmse: 0.0677 - val_loss: 0.0044 - val_rmse: 0.0666 - 4s/epoch - 3ms/step\n",
            "Epoch 57/100\n",
            "1586/1586 - 4s - loss: 0.0043 - rmse: 0.0658 - val_loss: 0.0042 - val_rmse: 0.0645 - 4s/epoch - 3ms/step\n",
            "Epoch 58/100\n",
            "1586/1586 - 4s - loss: 0.0041 - rmse: 0.0638 - val_loss: 0.0039 - val_rmse: 0.0628 - 4s/epoch - 3ms/step\n",
            "Epoch 59/100\n",
            "1586/1586 - 4s - loss: 0.0038 - rmse: 0.0619 - val_loss: 0.0037 - val_rmse: 0.0610 - 4s/epoch - 3ms/step\n",
            "Epoch 60/100\n",
            "1586/1586 - 4s - loss: 0.0036 - rmse: 0.0601 - val_loss: 0.0035 - val_rmse: 0.0590 - 4s/epoch - 2ms/step\n",
            "Epoch 61/100\n",
            "1586/1586 - 4s - loss: 0.0034 - rmse: 0.0584 - val_loss: 0.0033 - val_rmse: 0.0574 - 4s/epoch - 3ms/step\n",
            "Epoch 62/100\n",
            "1586/1586 - 4s - loss: 0.0032 - rmse: 0.0568 - val_loss: 0.0031 - val_rmse: 0.0558 - 4s/epoch - 3ms/step\n",
            "Epoch 63/100\n",
            "1586/1586 - 4s - loss: 0.0030 - rmse: 0.0551 - val_loss: 0.0029 - val_rmse: 0.0540 - 4s/epoch - 3ms/step\n",
            "Epoch 64/100\n",
            "1586/1586 - 4s - loss: 0.0029 - rmse: 0.0536 - val_loss: 0.0028 - val_rmse: 0.0527 - 4s/epoch - 3ms/step\n",
            "Epoch 65/100\n",
            "1586/1586 - 4s - loss: 0.0027 - rmse: 0.0522 - val_loss: 0.0026 - val_rmse: 0.0512 - 4s/epoch - 3ms/step\n",
            "Epoch 66/100\n",
            "1586/1586 - 4s - loss: 0.0026 - rmse: 0.0508 - val_loss: 0.0025 - val_rmse: 0.0499 - 4s/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "1586/1586 - 4s - loss: 0.0024 - rmse: 0.0494 - val_loss: 0.0024 - val_rmse: 0.0487 - 4s/epoch - 3ms/step\n",
            "Epoch 68/100\n",
            "1586/1586 - 4s - loss: 0.0023 - rmse: 0.0481 - val_loss: 0.0023 - val_rmse: 0.0475 - 4s/epoch - 3ms/step\n",
            "Epoch 69/100\n",
            "1586/1586 - 4s - loss: 0.0022 - rmse: 0.0468 - val_loss: 0.0021 - val_rmse: 0.0461 - 4s/epoch - 3ms/step\n",
            "Epoch 70/100\n",
            "1586/1586 - 4s - loss: 0.0021 - rmse: 0.0456 - val_loss: 0.0020 - val_rmse: 0.0450 - 4s/epoch - 3ms/step\n",
            "Epoch 71/100\n",
            "1586/1586 - 4s - loss: 0.0020 - rmse: 0.0445 - val_loss: 0.0019 - val_rmse: 0.0439 - 4s/epoch - 3ms/step\n",
            "Epoch 72/100\n",
            "1586/1586 - 4s - loss: 0.0019 - rmse: 0.0434 - val_loss: 0.0018 - val_rmse: 0.0427 - 4s/epoch - 3ms/step\n",
            "Epoch 73/100\n",
            "1586/1586 - 4s - loss: 0.0018 - rmse: 0.0423 - val_loss: 0.0017 - val_rmse: 0.0417 - 4s/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "1586/1586 - 4s - loss: 0.0017 - rmse: 0.0412 - val_loss: 0.0017 - val_rmse: 0.0407 - 4s/epoch - 3ms/step\n",
            "Epoch 75/100\n",
            "1586/1586 - 4s - loss: 0.0016 - rmse: 0.0402 - val_loss: 0.0016 - val_rmse: 0.0396 - 4s/epoch - 3ms/step\n",
            "Epoch 76/100\n",
            "1586/1586 - 4s - loss: 0.0015 - rmse: 0.0392 - val_loss: 0.0015 - val_rmse: 0.0386 - 4s/epoch - 3ms/step\n",
            "Epoch 77/100\n",
            "1586/1586 - 4s - loss: 0.0015 - rmse: 0.0383 - val_loss: 0.0014 - val_rmse: 0.0379 - 4s/epoch - 2ms/step\n",
            "Epoch 78/100\n",
            "1586/1586 - 4s - loss: 0.0014 - rmse: 0.0374 - val_loss: 0.0014 - val_rmse: 0.0369 - 4s/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "1586/1586 - 4s - loss: 0.0013 - rmse: 0.0365 - val_loss: 0.0013 - val_rmse: 0.0359 - 4s/epoch - 3ms/step\n",
            "Epoch 80/100\n",
            "1586/1586 - 4s - loss: 0.0013 - rmse: 0.0356 - val_loss: 0.0012 - val_rmse: 0.0350 - 4s/epoch - 3ms/step\n",
            "Epoch 81/100\n",
            "1586/1586 - 4s - loss: 0.0012 - rmse: 0.0348 - val_loss: 0.0012 - val_rmse: 0.0341 - 4s/epoch - 3ms/step\n",
            "Epoch 82/100\n",
            "1586/1586 - 4s - loss: 0.0012 - rmse: 0.0340 - val_loss: 0.0011 - val_rmse: 0.0334 - 4s/epoch - 3ms/step\n",
            "Epoch 83/100\n",
            "1586/1586 - 4s - loss: 0.0011 - rmse: 0.0332 - val_loss: 0.0011 - val_rmse: 0.0328 - 4s/epoch - 3ms/step\n",
            "Epoch 84/100\n",
            "1586/1586 - 4s - loss: 0.0011 - rmse: 0.0324 - val_loss: 0.0010 - val_rmse: 0.0320 - 4s/epoch - 3ms/step\n",
            "Epoch 85/100\n",
            "1586/1586 - 4s - loss: 0.0010 - rmse: 0.0317 - val_loss: 9.7416e-04 - val_rmse: 0.0312 - 4s/epoch - 3ms/step\n",
            "Epoch 86/100\n",
            "1586/1586 - 4s - loss: 9.6113e-04 - rmse: 0.0310 - val_loss: 9.2988e-04 - val_rmse: 0.0305 - 4s/epoch - 3ms/step\n",
            "Epoch 87/100\n",
            "1586/1586 - 4s - loss: 9.1954e-04 - rmse: 0.0303 - val_loss: 8.8856e-04 - val_rmse: 0.0298 - 4s/epoch - 3ms/step\n",
            "Epoch 88/100\n",
            "1586/1586 - 4s - loss: 8.7868e-04 - rmse: 0.0296 - val_loss: 8.5175e-04 - val_rmse: 0.0292 - 4s/epoch - 3ms/step\n",
            "Epoch 89/100\n",
            "1586/1586 - 4s - loss: 8.3995e-04 - rmse: 0.0290 - val_loss: 8.2018e-04 - val_rmse: 0.0286 - 4s/epoch - 3ms/step\n",
            "Epoch 90/100\n",
            "1586/1586 - 4s - loss: 8.0411e-04 - rmse: 0.0284 - val_loss: 7.8014e-04 - val_rmse: 0.0279 - 4s/epoch - 3ms/step\n",
            "Epoch 91/100\n",
            "1586/1586 - 4s - loss: 7.6942e-04 - rmse: 0.0277 - val_loss: 7.4851e-04 - val_rmse: 0.0274 - 4s/epoch - 3ms/step\n",
            "Epoch 92/100\n",
            "1586/1586 - 4s - loss: 7.3688e-04 - rmse: 0.0271 - val_loss: 7.1476e-04 - val_rmse: 0.0267 - 4s/epoch - 3ms/step\n",
            "Epoch 93/100\n",
            "1586/1586 - 4s - loss: 7.0458e-04 - rmse: 0.0265 - val_loss: 6.8705e-04 - val_rmse: 0.0262 - 4s/epoch - 3ms/step\n",
            "Epoch 94/100\n",
            "1586/1586 - 4s - loss: 6.7598e-04 - rmse: 0.0260 - val_loss: 6.5466e-04 - val_rmse: 0.0256 - 4s/epoch - 3ms/step\n",
            "Epoch 95/100\n",
            "1586/1586 - 4s - loss: 6.4759e-04 - rmse: 0.0254 - val_loss: 6.2714e-04 - val_rmse: 0.0250 - 4s/epoch - 3ms/step\n",
            "Epoch 96/100\n",
            "1586/1586 - 4s - loss: 6.2063e-04 - rmse: 0.0249 - val_loss: 6.0308e-04 - val_rmse: 0.0246 - 4s/epoch - 3ms/step\n",
            "Epoch 97/100\n",
            "1586/1586 - 4s - loss: 5.9470e-04 - rmse: 0.0244 - val_loss: 5.8379e-04 - val_rmse: 0.0242 - 4s/epoch - 3ms/step\n",
            "Epoch 98/100\n",
            "1586/1586 - 4s - loss: 5.7073e-04 - rmse: 0.0239 - val_loss: 5.5567e-04 - val_rmse: 0.0236 - 4s/epoch - 3ms/step\n",
            "Epoch 99/100\n",
            "1586/1586 - 4s - loss: 5.4779e-04 - rmse: 0.0234 - val_loss: 5.3128e-04 - val_rmse: 0.0230 - 4s/epoch - 3ms/step\n",
            "Epoch 100/100\n",
            "1586/1586 - 4s - loss: 5.2531e-04 - rmse: 0.0229 - val_loss: 5.1078e-04 - val_rmse: 0.0226 - 4s/epoch - 3ms/step\n",
            "3172/3172 - 6s - loss: 5.1078e-04 - rmse: 0.0226 - 6s/epoch - 2ms/step\n",
            "Final Training MSE: 0.0005107790348120034\n",
            "Final Training RMSE: 0.02260042168200016\n",
            "32/32 - 0s - loss: 5.1078e-04 - rmse: 0.0226 - 79ms/epoch - 2ms/step\n",
            "Final Validation MSE: 0.0005107790930196643\n",
            "Final Validation RMSE: 0.02260042168200016\n",
            "--- 14.59 minutes ---\n",
            "Data_BO30.mat\n",
            "(101500, 18) train samples\n",
            "(1015, 18) valid samples\n",
            "(101500, 2) train labels\n",
            "(1015, 2) valid labels\n",
            "Label Examples:\n",
            " [[ 0.22448 -0.22448]\n",
            " [-0.22448  0.22448]\n",
            " [-0.70711  0.70711]\n",
            " [-0.22448  0.22448]\n",
            " [-0.25882  0.96593]\n",
            " [ 0.70711 -0.70711]\n",
            " [ 0.96593  0.25882]\n",
            " [ 0.25882  0.96593]\n",
            " [-0.22448 -0.22448]]\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_30 (Dense)            (None, 50)                950       \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 50)                2550      \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 2)                 102       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,602\n",
            "Trainable params: 3,602\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "1586/1586 - 4s - loss: 0.3908 - rmse: 0.6252 - val_loss: 0.3683 - val_rmse: 0.6069 - 4s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1586/1586 - 4s - loss: 0.3568 - rmse: 0.5974 - val_loss: 0.3456 - val_rmse: 0.5879 - 4s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1586/1586 - 4s - loss: 0.3344 - rmse: 0.5783 - val_loss: 0.3227 - val_rmse: 0.5680 - 4s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1586/1586 - 4s - loss: 0.3112 - rmse: 0.5579 - val_loss: 0.2992 - val_rmse: 0.5470 - 4s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1586/1586 - 4s - loss: 0.2874 - rmse: 0.5361 - val_loss: 0.2749 - val_rmse: 0.5243 - 4s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1586/1586 - 4s - loss: 0.2644 - rmse: 0.5142 - val_loss: 0.2529 - val_rmse: 0.5029 - 4s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1586/1586 - 4s - loss: 0.2427 - rmse: 0.4927 - val_loss: 0.2327 - val_rmse: 0.4824 - 4s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1586/1586 - 4s - loss: 0.2223 - rmse: 0.4715 - val_loss: 0.2119 - val_rmse: 0.4603 - 4s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1586/1586 - 4s - loss: 0.2029 - rmse: 0.4505 - val_loss: 0.1928 - val_rmse: 0.4391 - 4s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1586/1586 - 4s - loss: 0.1847 - rmse: 0.4298 - val_loss: 0.1754 - val_rmse: 0.4188 - 4s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1586/1586 - 4s - loss: 0.1677 - rmse: 0.4095 - val_loss: 0.1597 - val_rmse: 0.3996 - 4s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1586/1586 - 4s - loss: 0.1520 - rmse: 0.3899 - val_loss: 0.1435 - val_rmse: 0.3789 - 4s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1586/1586 - 4s - loss: 0.1375 - rmse: 0.3708 - val_loss: 0.1304 - val_rmse: 0.3610 - 4s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1586/1586 - 4s - loss: 0.1241 - rmse: 0.3523 - val_loss: 0.1172 - val_rmse: 0.3423 - 4s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1586/1586 - 4s - loss: 0.1121 - rmse: 0.3348 - val_loss: 0.1057 - val_rmse: 0.3251 - 4s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1586/1586 - 4s - loss: 0.1012 - rmse: 0.3182 - val_loss: 0.0958 - val_rmse: 0.3096 - 4s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1586/1586 - 4s - loss: 0.0916 - rmse: 0.3026 - val_loss: 0.0863 - val_rmse: 0.2938 - 4s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1586/1586 - 4s - loss: 0.0830 - rmse: 0.2880 - val_loss: 0.0787 - val_rmse: 0.2806 - 4s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1586/1586 - 4s - loss: 0.0753 - rmse: 0.2744 - val_loss: 0.0719 - val_rmse: 0.2682 - 4s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1586/1586 - 4s - loss: 0.0684 - rmse: 0.2615 - val_loss: 0.0651 - val_rmse: 0.2551 - 4s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1586/1586 - 4s - loss: 0.0623 - rmse: 0.2496 - val_loss: 0.0591 - val_rmse: 0.2430 - 4s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1586/1586 - 4s - loss: 0.0568 - rmse: 0.2383 - val_loss: 0.0538 - val_rmse: 0.2320 - 4s/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "1586/1586 - 4s - loss: 0.0519 - rmse: 0.2278 - val_loss: 0.0493 - val_rmse: 0.2219 - 4s/epoch - 3ms/step\n",
            "Epoch 24/100\n",
            "1586/1586 - 4s - loss: 0.0475 - rmse: 0.2179 - val_loss: 0.0452 - val_rmse: 0.2125 - 4s/epoch - 3ms/step\n",
            "Epoch 25/100\n",
            "1586/1586 - 4s - loss: 0.0434 - rmse: 0.2084 - val_loss: 0.0412 - val_rmse: 0.2030 - 4s/epoch - 3ms/step\n",
            "Epoch 26/100\n",
            "1586/1586 - 4s - loss: 0.0399 - rmse: 0.1996 - val_loss: 0.0380 - val_rmse: 0.1950 - 4s/epoch - 3ms/step\n",
            "Epoch 27/100\n",
            "1586/1586 - 4s - loss: 0.0366 - rmse: 0.1913 - val_loss: 0.0349 - val_rmse: 0.1868 - 4s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1586/1586 - 4s - loss: 0.0336 - rmse: 0.1833 - val_loss: 0.0321 - val_rmse: 0.1793 - 4s/epoch - 3ms/step\n",
            "Epoch 29/100\n",
            "1586/1586 - 4s - loss: 0.0309 - rmse: 0.1757 - val_loss: 0.0295 - val_rmse: 0.1716 - 4s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1586/1586 - 4s - loss: 0.0284 - rmse: 0.1685 - val_loss: 0.0272 - val_rmse: 0.1649 - 4s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1586/1586 - 4s - loss: 0.0261 - rmse: 0.1617 - val_loss: 0.0249 - val_rmse: 0.1578 - 4s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1586/1586 - 4s - loss: 0.0241 - rmse: 0.1551 - val_loss: 0.0229 - val_rmse: 0.1515 - 4s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1586/1586 - 4s - loss: 0.0222 - rmse: 0.1489 - val_loss: 0.0211 - val_rmse: 0.1453 - 4s/epoch - 3ms/step\n",
            "Epoch 34/100\n",
            "1586/1586 - 4s - loss: 0.0205 - rmse: 0.1431 - val_loss: 0.0196 - val_rmse: 0.1400 - 4s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1586/1586 - 4s - loss: 0.0189 - rmse: 0.1376 - val_loss: 0.0180 - val_rmse: 0.1343 - 4s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1586/1586 - 4s - loss: 0.0175 - rmse: 0.1323 - val_loss: 0.0168 - val_rmse: 0.1295 - 4s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1586/1586 - 4s - loss: 0.0162 - rmse: 0.1273 - val_loss: 0.0156 - val_rmse: 0.1249 - 4s/epoch - 3ms/step\n",
            "Epoch 38/100\n",
            "1586/1586 - 4s - loss: 0.0150 - rmse: 0.1226 - val_loss: 0.0144 - val_rmse: 0.1201 - 4s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1586/1586 - 4s - loss: 0.0139 - rmse: 0.1181 - val_loss: 0.0133 - val_rmse: 0.1154 - 4s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1586/1586 - 4s - loss: 0.0130 - rmse: 0.1139 - val_loss: 0.0125 - val_rmse: 0.1119 - 4s/epoch - 3ms/step\n",
            "Epoch 41/100\n",
            "1586/1586 - 4s - loss: 0.0121 - rmse: 0.1099 - val_loss: 0.0116 - val_rmse: 0.1075 - 4s/epoch - 3ms/step\n",
            "Epoch 42/100\n",
            "1586/1586 - 4s - loss: 0.0113 - rmse: 0.1061 - val_loss: 0.0108 - val_rmse: 0.1041 - 4s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "1586/1586 - 4s - loss: 0.0105 - rmse: 0.1026 - val_loss: 0.0101 - val_rmse: 0.1007 - 4s/epoch - 3ms/step\n",
            "Epoch 44/100\n",
            "1586/1586 - 4s - loss: 0.0098 - rmse: 0.0991 - val_loss: 0.0095 - val_rmse: 0.0973 - 4s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "1586/1586 - 4s - loss: 0.0092 - rmse: 0.0959 - val_loss: 0.0088 - val_rmse: 0.0940 - 4s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "1586/1586 - 4s - loss: 0.0086 - rmse: 0.0927 - val_loss: 0.0083 - val_rmse: 0.0909 - 4s/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "1586/1586 - 4s - loss: 0.0081 - rmse: 0.0898 - val_loss: 0.0078 - val_rmse: 0.0881 - 4s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "1586/1586 - 4s - loss: 0.0076 - rmse: 0.0870 - val_loss: 0.0073 - val_rmse: 0.0854 - 4s/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "1586/1586 - 4s - loss: 0.0071 - rmse: 0.0843 - val_loss: 0.0069 - val_rmse: 0.0829 - 4s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "1586/1586 - 4s - loss: 0.0067 - rmse: 0.0817 - val_loss: 0.0064 - val_rmse: 0.0801 - 4s/epoch - 3ms/step\n",
            "Epoch 51/100\n",
            "1586/1586 - 4s - loss: 0.0063 - rmse: 0.0793 - val_loss: 0.0061 - val_rmse: 0.0778 - 4s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "1586/1586 - 4s - loss: 0.0059 - rmse: 0.0769 - val_loss: 0.0057 - val_rmse: 0.0756 - 4s/epoch - 3ms/step\n",
            "Epoch 53/100\n",
            "1586/1586 - 4s - loss: 0.0056 - rmse: 0.0747 - val_loss: 0.0054 - val_rmse: 0.0733 - 4s/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "1586/1586 - 4s - loss: 0.0053 - rmse: 0.0725 - val_loss: 0.0050 - val_rmse: 0.0710 - 4s/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "1586/1586 - 4s - loss: 0.0050 - rmse: 0.0704 - val_loss: 0.0048 - val_rmse: 0.0692 - 4s/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "1586/1586 - 4s - loss: 0.0047 - rmse: 0.0685 - val_loss: 0.0045 - val_rmse: 0.0673 - 4s/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "1586/1586 - 4s - loss: 0.0044 - rmse: 0.0665 - val_loss: 0.0043 - val_rmse: 0.0654 - 4s/epoch - 3ms/step\n",
            "Epoch 58/100\n",
            "1586/1586 - 4s - loss: 0.0042 - rmse: 0.0647 - val_loss: 0.0040 - val_rmse: 0.0635 - 4s/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "1586/1586 - 4s - loss: 0.0040 - rmse: 0.0630 - val_loss: 0.0039 - val_rmse: 0.0623 - 4s/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "1586/1586 - 4s - loss: 0.0038 - rmse: 0.0613 - val_loss: 0.0036 - val_rmse: 0.0602 - 4s/epoch - 3ms/step\n",
            "Epoch 61/100\n",
            "1586/1586 - 4s - loss: 0.0036 - rmse: 0.0596 - val_loss: 0.0034 - val_rmse: 0.0587 - 4s/epoch - 3ms/step\n",
            "Epoch 62/100\n",
            "1586/1586 - 4s - loss: 0.0034 - rmse: 0.0581 - val_loss: 0.0033 - val_rmse: 0.0573 - 4s/epoch - 3ms/step\n",
            "Epoch 63/100\n",
            "1586/1586 - 4s - loss: 0.0032 - rmse: 0.0565 - val_loss: 0.0031 - val_rmse: 0.0557 - 4s/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "1586/1586 - 4s - loss: 0.0030 - rmse: 0.0551 - val_loss: 0.0029 - val_rmse: 0.0542 - 4s/epoch - 3ms/step\n",
            "Epoch 65/100\n",
            "1586/1586 - 4s - loss: 0.0029 - rmse: 0.0537 - val_loss: 0.0028 - val_rmse: 0.0527 - 4s/epoch - 3ms/step\n",
            "Epoch 66/100\n",
            "1586/1586 - 4s - loss: 0.0027 - rmse: 0.0523 - val_loss: 0.0026 - val_rmse: 0.0514 - 4s/epoch - 3ms/step\n",
            "Epoch 67/100\n",
            "1586/1586 - 4s - loss: 0.0026 - rmse: 0.0510 - val_loss: 0.0025 - val_rmse: 0.0502 - 4s/epoch - 3ms/step\n",
            "Epoch 68/100\n",
            "1586/1586 - 4s - loss: 0.0025 - rmse: 0.0497 - val_loss: 0.0024 - val_rmse: 0.0491 - 4s/epoch - 3ms/step\n",
            "Epoch 69/100\n",
            "1586/1586 - 4s - loss: 0.0024 - rmse: 0.0485 - val_loss: 0.0023 - val_rmse: 0.0478 - 4s/epoch - 3ms/step\n",
            "Epoch 70/100\n",
            "1586/1586 - 4s - loss: 0.0022 - rmse: 0.0473 - val_loss: 0.0022 - val_rmse: 0.0466 - 4s/epoch - 3ms/step\n",
            "Epoch 71/100\n",
            "1586/1586 - 4s - loss: 0.0021 - rmse: 0.0462 - val_loss: 0.0021 - val_rmse: 0.0458 - 4s/epoch - 3ms/step\n",
            "Epoch 72/100\n",
            "1586/1586 - 4s - loss: 0.0020 - rmse: 0.0451 - val_loss: 0.0020 - val_rmse: 0.0445 - 4s/epoch - 3ms/step\n",
            "Epoch 73/100\n",
            "1586/1586 - 4s - loss: 0.0019 - rmse: 0.0440 - val_loss: 0.0019 - val_rmse: 0.0432 - 4s/epoch - 3ms/step\n",
            "Epoch 74/100\n",
            "1586/1586 - 4s - loss: 0.0018 - rmse: 0.0429 - val_loss: 0.0018 - val_rmse: 0.0423 - 4s/epoch - 3ms/step\n",
            "Epoch 75/100\n",
            "1586/1586 - 4s - loss: 0.0018 - rmse: 0.0419 - val_loss: 0.0017 - val_rmse: 0.0412 - 4s/epoch - 3ms/step\n",
            "Epoch 76/100\n",
            "1586/1586 - 4s - loss: 0.0017 - rmse: 0.0409 - val_loss: 0.0016 - val_rmse: 0.0404 - 4s/epoch - 3ms/step\n",
            "Epoch 77/100\n",
            "1586/1586 - 4s - loss: 0.0016 - rmse: 0.0400 - val_loss: 0.0016 - val_rmse: 0.0396 - 4s/epoch - 3ms/step\n",
            "Epoch 78/100\n",
            "1586/1586 - 4s - loss: 0.0015 - rmse: 0.0390 - val_loss: 0.0015 - val_rmse: 0.0385 - 4s/epoch - 3ms/step\n",
            "Epoch 79/100\n",
            "1586/1586 - 4s - loss: 0.0015 - rmse: 0.0382 - val_loss: 0.0014 - val_rmse: 0.0375 - 4s/epoch - 3ms/step\n",
            "Epoch 80/100\n",
            "1586/1586 - 4s - loss: 0.0014 - rmse: 0.0373 - val_loss: 0.0013 - val_rmse: 0.0367 - 4s/epoch - 3ms/step\n",
            "Epoch 81/100\n",
            "1586/1586 - 4s - loss: 0.0013 - rmse: 0.0364 - val_loss: 0.0013 - val_rmse: 0.0359 - 4s/epoch - 3ms/step\n",
            "Epoch 82/100\n",
            "1586/1586 - 4s - loss: 0.0013 - rmse: 0.0356 - val_loss: 0.0012 - val_rmse: 0.0351 - 4s/epoch - 2ms/step\n",
            "Epoch 83/100\n",
            "1586/1586 - 4s - loss: 0.0012 - rmse: 0.0348 - val_loss: 0.0012 - val_rmse: 0.0343 - 4s/epoch - 3ms/step\n",
            "Epoch 84/100\n",
            "1586/1586 - 4s - loss: 0.0012 - rmse: 0.0340 - val_loss: 0.0011 - val_rmse: 0.0336 - 4s/epoch - 2ms/step\n",
            "Epoch 85/100\n",
            "1586/1586 - 4s - loss: 0.0011 - rmse: 0.0333 - val_loss: 0.0011 - val_rmse: 0.0327 - 4s/epoch - 3ms/step\n",
            "Epoch 86/100\n",
            "1586/1586 - 4s - loss: 0.0011 - rmse: 0.0325 - val_loss: 0.0010 - val_rmse: 0.0321 - 4s/epoch - 2ms/step\n",
            "Epoch 87/100\n",
            "1586/1586 - 4s - loss: 0.0010 - rmse: 0.0318 - val_loss: 9.8041e-04 - val_rmse: 0.0313 - 4s/epoch - 3ms/step\n",
            "Epoch 88/100\n",
            "1586/1586 - 4s - loss: 9.6823e-04 - rmse: 0.0311 - val_loss: 9.4152e-04 - val_rmse: 0.0307 - 4s/epoch - 3ms/step\n",
            "Epoch 89/100\n",
            "1586/1586 - 4s - loss: 9.2760e-04 - rmse: 0.0305 - val_loss: 9.0155e-04 - val_rmse: 0.0300 - 4s/epoch - 3ms/step\n",
            "Epoch 90/100\n",
            "1586/1586 - 4s - loss: 8.8723e-04 - rmse: 0.0298 - val_loss: 8.6712e-04 - val_rmse: 0.0294 - 4s/epoch - 3ms/step\n",
            "Epoch 91/100\n",
            "1586/1586 - 4s - loss: 8.4930e-04 - rmse: 0.0291 - val_loss: 8.2208e-04 - val_rmse: 0.0287 - 4s/epoch - 3ms/step\n",
            "Epoch 92/100\n",
            "1586/1586 - 4s - loss: 8.1366e-04 - rmse: 0.0285 - val_loss: 7.9176e-04 - val_rmse: 0.0281 - 4s/epoch - 2ms/step\n",
            "Epoch 93/100\n",
            "1586/1586 - 4s - loss: 7.7884e-04 - rmse: 0.0279 - val_loss: 7.6051e-04 - val_rmse: 0.0276 - 4s/epoch - 2ms/step\n",
            "Epoch 94/100\n",
            "1586/1586 - 4s - loss: 7.4631e-04 - rmse: 0.0273 - val_loss: 7.3072e-04 - val_rmse: 0.0270 - 4s/epoch - 2ms/step\n",
            "Epoch 95/100\n",
            "1586/1586 - 4s - loss: 7.1529e-04 - rmse: 0.0267 - val_loss: 6.9681e-04 - val_rmse: 0.0264 - 4s/epoch - 2ms/step\n",
            "Epoch 96/100\n",
            "1586/1586 - 4s - loss: 6.8483e-04 - rmse: 0.0262 - val_loss: 6.6791e-04 - val_rmse: 0.0258 - 4s/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "1586/1586 - 4s - loss: 6.5577e-04 - rmse: 0.0256 - val_loss: 6.4289e-04 - val_rmse: 0.0254 - 4s/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "1586/1586 - 4s - loss: 6.2931e-04 - rmse: 0.0251 - val_loss: 6.1404e-04 - val_rmse: 0.0248 - 4s/epoch - 3ms/step\n",
            "Epoch 99/100\n",
            "1586/1586 - 4s - loss: 6.0337e-04 - rmse: 0.0246 - val_loss: 5.8445e-04 - val_rmse: 0.0242 - 4s/epoch - 2ms/step\n",
            "Epoch 100/100\n",
            "1586/1586 - 4s - loss: 5.7813e-04 - rmse: 0.0240 - val_loss: 5.6452e-04 - val_rmse: 0.0238 - 4s/epoch - 3ms/step\n",
            "3172/3172 - 6s - loss: 5.6452e-04 - rmse: 0.0238 - 6s/epoch - 2ms/step\n",
            "Final Training MSE: 0.0005645208293572068\n",
            "Final Training RMSE: 0.023759646341204643\n",
            "32/32 - 0s - loss: 5.6452e-04 - rmse: 0.0238 - 76ms/epoch - 2ms/step\n",
            "Final Validation MSE: 0.0005645216442644596\n",
            "Final Validation RMSE: 0.023759663105010986\n",
            "--- 22.06 minutes ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import scipy.io as spio\n",
        "import math\n",
        "import time\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "num_classes = 2\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "BOs = [\"1\", \"7\", \"30\"] #backoffs\n",
        "\n",
        "for bo in BOs: \n",
        "    # Load the data\n",
        "\n",
        "    matname = \"Data_BO\" + bo + \".mat\"\n",
        "    print(matname)\n",
        "    \n",
        "    mat = spio.loadmat(matname, squeeze_me=True)\n",
        "    x_train = mat['x_train']\n",
        "    x_valid = mat['data_val']\n",
        "    x_test = mat['x_pred']\n",
        "    y_train = mat['y_train']\n",
        "    y_valid = mat['target_val']\n",
        "    #y_test = mat['y_test']\n",
        "\n",
        "\n",
        "    # Convert the data to floats between 0 and 1.\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_valid = x_valid.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "\n",
        "    print(x_train.shape, 'train samples')\n",
        "    print(x_valid.shape, 'valid samples')\n",
        "    #print(x_test.shape, 'test samples')\n",
        "    print(y_train.shape, 'train labels')\n",
        "    print(y_valid.shape, 'valid labels')\n",
        "    #print(y_test.shape, 'test labels')\n",
        "    print('Label Examples:\\n', y_train[0:9]);\n",
        "\n",
        "    # Formatting\n",
        "    fmtLen = int(math.ceil(math.log(max(batch_size, y_valid.shape[0]),10)))\n",
        "\n",
        "    # Define the network\n",
        "    model = Sequential()\n",
        "    model.add(Dense(50, activation='tanh', input_dim=18))\n",
        "    model.add(Dense(50, activation='tanh'))\n",
        "    model.add(Dense(num_classes, activation='linear'))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(loss=keras.metrics.mean_squared_error,\n",
        "                optimizer=SGD(),\n",
        "                metrics=[keras.metrics.RootMeanSquaredError(name='rmse')])\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs,\n",
        "                        verbose=2,\n",
        "                        validation_data=(x_valid, y_valid))\n",
        "\n",
        "    score = model.evaluate(x_train, y_train, verbose=2)\n",
        "    print('Final Training MSE:', score[0])\n",
        "    print('Final Training RMSE:', score[1])\n",
        "\n",
        "    score = model.evaluate(x_valid, y_valid, verbose=2)\n",
        "    print('Final Validation MSE:', score[0])\n",
        "    print('Final Validation RMSE:', score[1])\n",
        "\n",
        "    #score = model.evaluate(x_test, y_test, verbose=1)\n",
        "    #print('Test MSE:', score[0])\n",
        "    #print('Test RMSE:', score[1])\n",
        "\n",
        "\n",
        "    predictions = model.predict(x_test)\n",
        "    matname = \"predictionsBO\" + bo + \".mat\"\n",
        "    spio.savemat(matname, {'pred': predictions})\n",
        "    savename = \"deep_model_BO\" + bo + \".h5\"\n",
        "    model.save(savename)\n",
        "\n",
        "\n",
        "    elapsedTime = (time.time() - start_time)/60\n",
        "    print(\"--- %.2f minutes ---\" % elapsedTime)\n",
        "\n",
        "    '''\n",
        "    # summarize history for accuracy\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    figname = 'lossPlot'+snr+'.png'\n",
        "    plt.savefig(figname)\n",
        "    '''"
      ]
    }
  ]
}